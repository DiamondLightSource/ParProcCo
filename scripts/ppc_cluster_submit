#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations

import argparse
import logging
import os
from typing import List

from ParProcCo.job_controller import JobController
from ParProcCo.passthru_wrapper import PassThruWrapper

def create_parser():
    '''
     $ ppc_cluster_submit program [--output cluster_output_dir] [--jobs 4] --cores 6 --memory 4G -s 0.01 ... [input files]
     '''
    parser = argparse.ArgumentParser(description='ParProcCo run script')
    parser.add_argument('-o', '--output', help='str: cluster output file or directory')
    parser.add_argument('--jobs', help='int: number of cluster jobs to split processing into', type=int, default=1)
    parser.add_argument('--memory', help='str: memory to use per cluster job', required=True)
    parser.add_argument('--cores', help='int: number of cores to use per cluster job', type=int, required=True)
    return parser

def run_ppc(args: argparse.Namespace, script_args: List) -> None:
    '''
    Run JobController
    '''
    beamline = os.getenv('BEAMLINE')
    if not beamline:
        raise ValueError('BEAMLINE environment variable not defined')
    cluster = os.getenv('SGE_CELL')
    if not cluster:
        raise ValueError('SGE_CELL environment variable not defined. Module load global/cluster or hamilton')
    if cluster == 'HAMILTON':
        cluster_queue = 'all.q'
        cluster_resources = None
    elif cluster == 'DLS_SCIENCE':
        from getpass import getuser
        if getuser() == 'gda2':
            cluster_queue = 'high.q'
            logging.debug('User is gda2 so using cluster queue %s', cluster_queue)
        else:
            cluster_queue = 'medium.q'
        cluster_resources = {"cpu_model": "intel-xeon"}
    else:
        raise ValueError('SGE_CELL value not known (HAMILTON or DLS_SCIENCE)')

    logging.info('Running for beamline %s on cluster %s in queue %s with resources %s', beamline, cluster, cluster_queue, cluster_resources)
    program = script_args[0]
    if args.jobs == 1:
        wrapper = PassThruWrapper()
    elif args.jobs > 1:
        from ParProcCo.utils import get_allowed_programs
        allowed, cfg = get_allowed_programs()
        if program not in allowed:
            raise ValueError(f'{program} not on allowed list in {cfg}')

        import importlib
        try:
            package = allowed[program]
            wrapper_module = importlib.import_module(f'{package}.{program}_wrapper')
        except Exception as exc:
            raise ValueError(f'Cannot import {program}_wrapper as a Python module from package {package}') from exc
        try:
            wrapper = wrapper_module.Wrapper()
        except Exception as exc:
            raise ValueError(f'Cannot create Wrapper from {program}_wrapper module') from exc
    else:
        raise ValueError(f'Number of jobs must be one or more, given {args.jobs}')
    wrapper.set_cores(args.cores)
    output = wrapper.get_output(args.output, script_args[1:])
    jc = JobController(wrapper, output, beamline, cluster_queue, cluster_resources)
    jc.run(args.jobs, script_args, args.memory, "PPC-" + program)
    print("complete")

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    args, script_args = create_parser().parse_known_args()
    run_ppc(args, script_args)
