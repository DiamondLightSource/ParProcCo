#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations

import argparse
import logging
import os
from typing import List

from ParProcCo.job_controller import JobController
from ParProcCo.simple_data_slicer import SimpleDataSlicer
from ParProcCo.msm_processing_mode import MSMProcessingMode
from ParProcCo.nxdata_aggregation_mode import NXdataAggregationMode
from ParProcCo.passthru_processing_mode import PassThruProcessingMode
from ParProcCo.program_wrapper import ProgramWrapper

def create_parser():
    '''
     $ ppc_cluster_submit rs_map [--output cluster_output_dir] [--jobs 4] --cores 6 --memory 4G -s 0.01 ... [input files]
     '''
    parser = argparse.ArgumentParser(description='ParProcCo run script')
    parser.add_argument('-o', '--output', help='str: cluster output file or directory')
    parser.add_argument('--jobs', help='int: number of cluster jobs to split processing into', type=int, default=1)
    parser.add_argument('--memory', help='str: memory to use per cluster job', required=True)
    parser.add_argument('--cores', help='int: number of cores to use per cluster job', type=int, required=True)
    return parser

def run_ppc(args: argparse.Namespace, script_args: List) -> None:
    '''
    Run JobController
    '''
    beamline = os.getenv('BEAMLINE')
    if not beamline:
        raise ValueError('BEAMLINE environment variable not defined')
    cluster = os.getenv('SGE_CELL')
    if not cluster:
        raise ValueError('SGE_CELL environment variable not defined. Module load global/cluster or hamilton')
    if cluster == 'HAMILTON':
        cluster_queue = 'all.q'
        cluster_resources = None
    elif cluster == 'DLS_SCIENCE':
        from getpass import getuser
        if getuser() == 'gda2':
            cluster_queue = 'high.q'
            logging.debug('User is gda2 so using cluster queue {}', cluster_queue)
        else:
            cluster_queue = 'medium.q'
        cluster_resources = {"cpu_model": "intel-xeon"}
    else:
        raise ValueError('SGE_CELL value not known (HAMILTON or DLS_SCIENCE)')

    logging.info('Running for beamline {} on cluster {} in queue {} with resources {}', beamline, cluster, cluster_queue, cluster_resources)
    program = script_args[0]
    if args.jobs == 1:
        wrapper = ProgramWrapper(PassThruProcessingMode())
    elif args.jobs > 1:
        if program =='rs_map':
            wrapper = ProgramWrapper(MSMProcessingMode(), SimpleDataSlicer(), NXdataAggregationMode())
        else:
            raise ValueError(f'Program submitted {program} not supported for more than one job')
    else:
        raise ValueError(f'Number of jobs must be one or more, given {args.jobs}')

    output = wrapper.get_output(args.output, script_args)
    jc = JobController(wrapper, output, beamline, cluster_queue, cluster_resources)
    jc.run(args.jobs, script_args, args.memory, args.cores, "PPC-" + program)
    print("complete")

if __name__ == '__main__':
    args, script_args = create_parser().parse_known_args()
    run_ppc(args, script_args)
